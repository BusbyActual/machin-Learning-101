### Reducing loss

Derivative of (y-y)2 tells us how loss changes for an example

We take small steps to minimize loss
 - we call these gradient steps(they're really negative gradient steps_)
 - this optimization strategy is called gradient descent

 ### Gradient descent



 ### Learning rate

    - the loss function takes two input values
    y': The model's prediction for features x
    y: The correct label corresponding to features x.

    Hyperparamaters are the knobs that programers tweak in ML

     Learning rate low - takes forever ( lots of tiny adjustments )
     Learning rate high - innaccurate ( overshoots the minimum giant steps potentially in wrong direction)


 ### Weight initialization

 - for convex problems wieghts can start anywhere
  - bowl shape
   - just on minimum

  - For neural nets
  - non-convew like egg crate
  - more than one minimum
  - strong dependency on intial values

  SGD & mini-batch gradient descent

  Could compute gradient over entire data set on each step
   = unnecessary

  Stochastic Gradient Descent - one exampel at a time.
  Mini-Batch Gradient Descent - batches of 10-1000
   - loss and gradients averaged over the batch(hybrid approach tot he above)

### Reducing loss

Derivative of (y-y)2 tells us how loss changes for an example

We take small steps to minimize loss
 - we call these gradient steps(they're really negative gradient steps_)
 - this optimization strategy is called gradient descent
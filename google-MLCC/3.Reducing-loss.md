### Reducing loss

Derivative of (y-y)2 tells us how loss changes for an example

We take small steps to minimize loss
 - we call these gradient steps(they're really negative gradient steps_)
 - this optimization strategy is called gradient descent

 ### Gradient descent

    - the loss function takes two input values
    y': The model's prediction for features x
    y: The correct label corresponding to features x.

    Hyperparamaters are the knobs that programers tweak in ML

     Learning rate low - takes forever ( lots of tiny adjustments )
     Learning rate high - innaccurate ( overshoots the minimum giant steps potentially in wrong direction)


     ###weight initialization